{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Ques_1.ipynb\n",
        "# --------------------------------------\n",
        "# Data Quality Assessment & Profiling\n",
        "# --------------------------------------\n",
        "\n",
        "# Step 1: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: For full profiling report\n",
        "# !pip install ydata-profiling\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Step 2: Load the Dataset\n",
        "# Replace 'your_dataset.csv' with your actual dataset path\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Step 3: Initial Data Exploration\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset shape (rows, columns):\", df.shape)\n",
        "\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(df.describe(include='all'))\n",
        "\n",
        "# Step 4: Missing Values Check\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nPercentage of missing values per column:\")\n",
        "print(df.isnull().mean() * 100)\n",
        "\n",
        "# Visualize missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
        "plt.title(\"Heatmap of Missing Values\")\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Duplicate Data\n",
        "num_duplicates = df.duplicated().sum()\n",
        "print(f\"\\nNumber of duplicate rows: {num_duplicates}\")\n",
        "\n",
        "# Removing duplicates\n",
        "df = df.drop_duplicates()\n",
        "print(\"Duplicates removed.\")\n",
        "\n",
        "# Step 6: Categorical Consistency Check\n",
        "print(\"\\nUnique values in categorical columns:\")\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    print(f\"{col}: {df[col].unique()}\")\n",
        "\n",
        "# Optional: Standardizing categorical values\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col] = df[col].str.lower().str.strip()\n",
        "\n",
        "# Step 7: Outlier Detection (Numerical Columns)\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "for col in numerical_cols:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'Boxplot for {col}')\n",
        "    plt.show()\n",
        "\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "\n",
        "    print(f\"Column '{col}' has {outliers.shape[0]} potential outliers.\")\n",
        "\n",
        "# Step 8: Generate Data Profiling Report (Optional)\n",
        "profile = ProfileReport(df, title=\"Data Quality Profiling Report\", explorative=True)\n",
        "profile.to_file(\"data_profiling_report.html\")\n",
        "\n",
        "print(\"\\nData profiling report generated: data_profiling_report.html\")"
      ],
      "metadata": {
        "id": "6wrrX6RGP_n5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}