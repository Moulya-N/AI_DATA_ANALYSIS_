{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Autoencoders for Anomaly Detection - Complete Code\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Load dataset (Credit Card Fraud Detection as example)\n",
        "url = \"https://www.dropbox.com/s/sf3jwv3z1ebq1tl/creditcard.csv?dl=1\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Drop 'Time' column and extract labels\n",
        "df = df.drop(['Time'], axis=1)\n",
        "labels = df['Class'].values\n",
        "features = df.drop(['Class'], axis=1)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use only normal (non-fraud) data to train the autoencoder\n",
        "X_train = X_train[y_train == 0]\n",
        "\n",
        "# Define autoencoder architecture\n",
        "input_dim = X_train.shape[1]\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(14, activation='relu', activity_regularizer=regularizers.l1(1e-5))(input_layer)\n",
        "encoded = Dense(7, activation='relu')(encoded)\n",
        "decoded = Dense(14, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train autoencoder\n",
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict on test data\n",
        "reconstructions = autoencoder.predict(X_test)\n",
        "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
        "\n",
        "# Set threshold for anomaly detection\n",
        "threshold = np.percentile(mse[y_test == 0], 95)\n",
        "print(f\"Reconstruction error threshold: {threshold}\")\n",
        "\n",
        "# Detect anomalies\n",
        "y_pred = mse > threshold\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title(\"Autoencoder Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RT5dKrEZ6sMz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}