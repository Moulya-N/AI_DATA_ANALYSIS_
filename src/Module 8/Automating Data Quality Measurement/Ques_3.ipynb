{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 2: Load and Inspect the Dataset\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your actual file path\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Display dataset information\n",
        "print(data.info())\n",
        "\n",
        "# Check for missing values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Step 3: Data Preprocessing (Handling Missing Values & Selecting Numerical Features)\n",
        "# Drop rows with missing values\n",
        "data_cleaned = data.dropna()\n",
        "\n",
        "# Select numerical features for anomaly detection\n",
        "numerical_features = data_cleaned.select_dtypes(include=[np.number])\n",
        "\n",
        "# Display statistical summary of numerical features\n",
        "print(numerical_features.describe())\n",
        "\n",
        "# Step 4: Feature Scaling (Optional, if features vary in scale)\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(numerical_features)\n",
        "\n",
        "# Convert scaled data back into DataFrame\n",
        "scaled_data = pd.DataFrame(scaled_features, columns=numerical_features.columns)\n",
        "\n",
        "# Step 5: Apply Isolation Forest for Anomaly Detection\n",
        "# Initialize the Isolation Forest model\n",
        "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "iso_forest.fit(scaled_data)\n",
        "\n",
        "# Predict anomalies\n",
        "anomaly_labels = iso_forest.predict(scaled_data)\n",
        "\n",
        "# Add the anomaly labels to the original data\n",
        "data_cleaned['Anomaly'] = anomaly_labels\n",
        "\n",
        "# Step 6: Visualize Anomalies\n",
        "# Separate anomalies and normal data for visualization\n",
        "anomalies = data_cleaned[data_cleaned['Anomaly'] == -1]\n",
        "normal_data = data_cleaned[data_cleaned['Anomaly'] == 1]\n",
        "\n",
        "# Plot anomalies vs normal data (adjust 'Feature1' and 'Feature2' to actual feature names)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(normal_data['Feature1'], normal_data['Feature2'], c='blue', label='Normal')\n",
        "plt.scatter(anomalies['Feature1'], anomalies['Feature2'], c='red', label='Anomaly')\n",
        "plt.xlabel('Feature1')\n",
        "plt.ylabel('Feature2')\n",
        "plt.title('Anomaly Detection using Isolation Forest')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Evaluate the Results (if ground truth labels are available)\n",
        "# Assuming you have true labels for anomalies (e.g., 'TrueLabel')\n",
        "true_labels = data_cleaned['TrueLabel']  # Replace with your actual true label column\n",
        "\n",
        "# Map Isolation Forest anomaly labels to match the true labels (1: Anomaly, 0: Normal)\n",
        "predicted_labels = np.where(data_cleaned['Anomaly'] == -1, 1, 0)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "RJjnM9MzVg7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}